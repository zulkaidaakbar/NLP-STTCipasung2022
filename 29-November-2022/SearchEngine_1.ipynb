{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMneMdUHR9on5F0vm85zKzB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["We will first retrieve a number of topics from the Wikipedia category tree. We will use the Wikimedia API for this. The code will write out all Wikipedia categories for which at least 200 documents exist, creating the file wiki_categories.txt. (It takes a minute or two, so be patient! "],"metadata":{"id":"E8vM3OUOX1aw"}},{"cell_type":"code","execution_count":31,"metadata":{"id":"5cEB4CI0atXt","executionInfo":{"status":"ok","timestamp":1669685663842,"user_tz":300,"elapsed":25275,"user":{"displayName":"zulkaida akbar","userId":"03264843683657800588"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7b7305dc-6732-433f-c916-b7ec55c155e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import requests\n","import re\n","\n","S = requests.Session()\n","\n","URL = \"https://en.wikipedia.org/w/api.php\"\n","\n","PARAMS = {\n","    \"action\": \"query\",\n","    \"format\": \"json\",\n","    \"list\": \"allcategories\",\n","    \"acmin\": 200,\n","    \"aclimit\": 500\n","}\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","f = open(\"./drive/MyDrive/Colab-Notebooks/Search-Engine/wiki_categories.txt\",'w')\n","\n","for i in range(100):\n","    R = S.get(url=URL, params=PARAMS)\n","    DATA = R.json()\n","\n","    CATEGORIES = DATA[\"query\"][\"allcategories\"]\n","\n","    for cat in CATEGORIES:\n","        cat_name = cat[\"*\"]\n","        m = re.search(\"[0-9]{4}\",cat_name)\n","        #if cat_name[-6:] not in ['births','deaths']:\n","        if not m:\n","            f.write(cat_name+'\\n')\n","    \n","    if \"continue\" in DATA:\n","        PARAMS[\"acfrom\"] = DATA[\"continue\"][\"accontinue\"]\n","    else:\n","        break\n","\n","f.close()"]},{"cell_type":"markdown","source":["Open your wiki_categories.txt file and scroll through it. Select 10 categories from it. Make sure you don't select categories which look like Wikipedia-internal labels (e.g. Wikipedia books', Pages with script errors', etc). Copy and paste those category names into a file named tmp_cat.txt in the top directory of the repo. We will now retrieve Wikipedia pages for our categories. We first need a list of page titles for each category. This will create a file titles.txt for each category in your data/ directory, containing 100 Wikipedia titles for each of the categories in your tmp_cat.txt file."],"metadata":{"id":"CuxrNgY9YJbo"}},{"cell_type":"code","source":["import requests\n","import os\n","\n","def read_categories():\n","    with open(\"./drive/MyDrive/Colab-Notebooks/Search-Engine/tmp_cat.txt\",'r') as f:\n","        categories = f.read().splitlines()\n","    return categories\n","\n","S = requests.Session()\n","\n","URL = \"https://en.wikipedia.org/w/api.php\"\n","\n","categories = read_categories()\n","print(categories)\n","\n","\n","for cat in categories:\n","    #cat_dir = \"data/categories/\"+cat.replace(' ','_')\n","    cat_dir = \"./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/\"+cat.replace(' ','_')\n","    if not os.path.isdir(cat_dir):\n","        os.mkdir(cat_dir)\n","    title_file = open(os.path.join(cat_dir,\"titles.txt\"),'w')\n","\n","    PARAMS = {\n","        \"action\": \"query\",\n","        \"list\": \"categorymembers\",\n","        \"format\": \"json\",\n","        \"cmtitle\": \"Category:\"+cat,\n","        \"cmlimit\": \"100\"\n","    }\n","\n","    for i in range(1):    #increase 1 to more to get additional data\n","        R = S.get(url=URL, params=PARAMS)\n","        DATA = R.json()\n","\n","        PAGES = DATA[\"query\"][\"categorymembers\"]\n","\n","        for page in PAGES:\n","            title = page[\"title\"]\n","            ID = str(page[\"pageid\"])\n","            if title[:9] != \"Category:\":\n","                title_file.write(ID+' '+title+'\\n')\n"," \n","        if \"continue\" in DATA:\n","            PARAMS[\"cmcontinue\"] = DATA[\"continue\"][\"cmcontinue\"]\n","        else:\n","            break\n","\n","    title_file.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ovwInaVab1BE","executionInfo":{"status":"ok","timestamp":1669685967880,"user_tz":300,"elapsed":1258,"user":{"displayName":"zulkaida akbar","userId":"03264843683657800588"}},"outputId":"f92c0eb6-71e0-4d65-f0f6-c120ee3508e1"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["['Brazilian telenovelas', 'Cold War films', 'Fauna of Zimbabwe', 'Freeware games', 'International law', 'Landscape painters', 'Members of the Chinese Academy of Sciences', 'Musicals based on novels', 'Particle physics', '21st-century women mathematicians']\n"]}]},{"cell_type":"markdown","source":["We now need to retrieve the text of those documents. To speed things up and in order not to use too much hard disk space, we will just retrieve the intro text of each document rather than the whole page. This will create a file linear.txt for each category in your data/ directory, containing the introductory text of each Wikipedia page listed in titles.txt files. "],"metadata":{"id":"KKKQZZhAYeMX"}},{"cell_type":"code","source":["import requests\n","import os\n","\n","def read_titles(filename):\n","    IDs = []\n","    titles = []\n","    f = open(filename,'r')\n","    for l in f:\n","        l.rstrip('\\n')\n","        IDs.append(l.split()[0])\n","        titles.append(' '.join(l.split()[1:]))\n","    return IDs,titles\n","\n","\n","def read_categories():\n","    with open(\"./drive/MyDrive/Colab-Notebooks/Search-Engine/tmp_cat.txt\",'r') as f:\n","        categories = f.read().splitlines()\n","    return categories\n","\n","\n","\n","S = requests.Session()\n","\n","URL = \"https://en.wikipedia.org/w/api.php\"\n","\n","\n","categories = read_categories()\n","\n","for cat in categories:\n","    print(\"Processing category\",cat)\n","    #cat_dir = \"data/categories/\"+cat.replace(' ','_')\n","    cat_dir = \"./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/\"+cat.replace(' ','_')\n","    title_file = os.path.join(cat_dir,\"titles.txt\")\n","    IDs, titles = read_titles(title_file)\n","\n","    content_file = open(os.path.join(cat_dir,\"linear.txt\"),'w')\n","\n","    for i in range(len(titles)):\n","        PARAMS = {\n","            \"action\": \"query\",\n","            \"prop\": \"extracts\",\n","            \"format\": \"json\",\n","            \"exintro\": True,\n","            \"explaintext\": True,\n","            \"redirects\": True,\n","            \"titles\": titles[i]\n","        }\n","\n","        R = S.get(url=URL, params=PARAMS)\n","        DATA = R.json()\n","\n","        PAGES = DATA[\"query\"][\"pages\"]\n","\n","        for page in PAGES:\n","            extract = PAGES[page][\"extract\"]\n","            content_file.write(\"<doc id=\\\"\"+IDs[i]+\"\\\" title=\\\"\"+titles[i]+\"\\\">\\n\")\n","            content_file.write(extract+'\\n')\n","            content_file.write(\"</doc>\\n\\n\")\n","\n","    content_file.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VTVmuxgRdtUE","executionInfo":{"status":"ok","timestamp":1669686226994,"user_tz":300,"elapsed":90483,"user":{"displayName":"zulkaida akbar","userId":"03264843683657800588"}},"outputId":"49df7c2e-584d-4f70-844f-dc0dd1e8f51a"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing category Brazilian telenovelas\n","Processing category Cold War films\n","Processing category Fauna of Zimbabwe\n","Processing category Freeware games\n","Processing category International law\n","Processing category Landscape painters\n","Processing category Members of the Chinese Academy of Sciences\n","Processing category Musicals based on novels\n","Processing category Particle physics\n","Processing category 21st-century women mathematicians\n"]}]},{"cell_type":"markdown","source":["Let's now convert our raw texts into sets of features. We will do this using a vocabulary of character ngrams."],"metadata":{"id":"Rf_VyY3FYs9I"}},{"cell_type":"code","source":["#USAGE: python3 ngrams.py <ngram size>\n","\n","import sys\n","from os import listdir\n","from os.path import isfile, isdir, join\n","    \n","\n","d = './drive/MyDrive/Colab-Notebooks/Search-Engine/Categories'\n","catdirs = [join(d,o) for o in listdir(d) if isdir(join(d,o))]\n","n = 6\n","print(catdirs)\n","for cat in catdirs:\n","    ngrams = {}\n","    f = open(join(cat,'linear.txt'),'r')\n","    for l in f:\n","        if \"<doc id\" not in l and \"</doc\" not in l:\n","            l = l.rstrip('\\n').lower()\n","            for i in range(len(l)-n+1):\n","                ngram = l[i:i+n]\n","                \n","                if ngram in ngrams:\n","                    ngrams[ngram]+=1\n","                else:\n","                    ngrams[ngram]=1\n","    f.close()\n","\n","    ngramfile = open(join(cat,\"linear.\"+str(n)+\".ngrams\"),'w')\n","    for k in sorted(ngrams, key=ngrams.get, reverse=True):\n","        ngramfile.write(k+'\\t'+str(ngrams[k])+'\\n')\n","    ngramfile.close() \n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5R56yJT_e-Dl","executionInfo":{"status":"ok","timestamp":1669686541492,"user_tz":300,"elapsed":859,"user":{"displayName":"zulkaida akbar","userId":"03264843683657800588"}},"outputId":"3c95a4dd-f5dd-4695-c3e4-db5a69ee742a"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["['./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Brazilian_telenovelas', './drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Cold_War_films', './drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Fauna_of_Zimbabwe', './drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Freeware_games', './drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/International_law', './drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Landscape_painters', './drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Members_of_the_Chinese_Academy_of_Sciences', './drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Musicals_based_on_novels', './drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Particle_physics', './drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/21st-century_women_mathematicians']\n"]}]},{"cell_type":"code","source":["import sys\n","import string\n","from math import log\n","from os import listdir\n","from os.path import isfile, isdir, join\n","\n","\n","d = './drive/MyDrive/Colab-Notebooks/Search-Engine/Categories'\n","catdirs = [join(d,o) for o in listdir(d) if isdir(join(d,o))]\n","\n","def contain_punctuation(s):\n","    punctuation = [c for c in string.punctuation]\n","    punctuation.append(' ')\n","    r = any(c in s for c in punctuation) \n","    return r\n","\n","def normalise_tfs(tfs,total):\n","    for k,v in tfs.items():\n","        tfs[k] = v / total\n","    return tfs\n","\n","def log_idfs(idfs,num_cats):\n","    for k,v in idfs.items():\n","        idfs[k] = log(num_cats / v)\n","    return idfs\n","\n","cat_tfs = {}\n","cat_tf_idfs = {}\n","idfs = {}\n","\n","for cat in catdirs:\n","    tfs = {}\n","    sum_freqs = 0\n","    #print(\"Processing\",filename,\"...\")\n","    ngram_files = [join(cat,f) for f in listdir(cat) if isfile(join(cat, f)) and '.ngrams' in f]\n","    for ngram_file in ngram_files:\n","        f = open(ngram_file,'r')\n","        for l in f:\n","            l = l.rstrip()\n","            ngram = '\\t'.join(i for i in l.split('\\t')[:-1])\n","            freq = int(l.split('\\t')[-1])\n","            tfs[ngram] = freq\n","            sum_freqs+=freq\n","            if ngram in idfs:\n","                idfs[ngram]+=1\n","            else:\n","                idfs[ngram]=1\n","        f.close()\n","\n","    tfs = normalise_tfs(tfs,sum_freqs)\n","    cat_tfs[cat] = tfs\n","\n","    #for k in sorted(idfs, key=tfs.get, reverse=True)[:10]:\n","    #    print(k,idfs[k])\n","\n","idfs = log_idfs(idfs, len(catdirs))\n","\n","vocab=[]\n","\n","for cat in catdirs:\n","    tf_idfs = {}\n","    tfs = cat_tfs[cat]\n","    for ngram,tf in tfs.items():\n","        tf_idfs[ngram] = tf * idfs[ngram]\n","    cat_tf_idfs[cat] = tf_idfs\n","\n","    c = 0\n","    for k in sorted(tf_idfs, key=tf_idfs.get, reverse=True):\n","        #only keep top 100 dimensions per category. Also, we won't keep ngrams with spaces\n","        if c == 100:\n","            break\n","        if k not in vocab and not contain_punctuation(k):\n","            vocab.append(k)\n","            c+=1\n","\n","print(\"VOCAB SIZE:\",len(vocab))\n","\n","#Write tf-idfs for each category\n","for cat in catdirs:\n","    tf_idfs = cat_tf_idfs[cat]\n","    f = open(join(cat,'tf_idfs.txt'),'w')\n","    for ngram in sorted(vocab):\n","        if ngram in tf_idfs:\n","            f.write(ngram+' '+str(tf_idfs[ngram])+'\\n')\n","        else:\n","            f.write(ngram+' 0.0\\n')\n","    f.close()\n"," \n","\n","vocab_file = open(\"./drive/MyDrive/Colab-Notebooks/Search-Engine/vocab_file.txt\",'w')\n","for ngram in sorted(vocab):\n","    vocab_file.write(ngram+'\\n')\n","vocab_file.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2yJsVk2cZzpD","executionInfo":{"status":"ok","timestamp":1669686705700,"user_tz":300,"elapsed":1495,"user":{"displayName":"zulkaida akbar","userId":"03264843683657800588"}},"outputId":"a525a295-f544-47b1-d9de-8d34c4883c2e"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["VOCAB SIZE: 1000\n"]}]},{"cell_type":"code","source":["#USAGE: python3 mk_cat_vectors.py\n","\n","import sys\n","import numpy as np\n","from os import listdir\n","from os.path import isfile, isdir, join\n","    \n","\n","def read_vocab2():\n","    i_to_ngrams = {}\n","    ngrams_to_i = {}\n","    c = 0\n","    f = open('./drive/MyDrive/Colab-Notebooks/Search-Engine/vocab_file.txt','r')\n","    for l in f:\n","        l = l.rstrip()\n","        i_to_ngrams[c] = l\n","        ngrams_to_i = c\n","        c+=1\n","    return i_to_ngrams, ngrams_to_i\n","\n","def read_vocab():\n","    with open('./drive/MyDrive/Colab-Notebooks/Search-Engine/vocab_file.txt','r') as f:\n","        vocab = f.read().splitlines()\n","    return vocab\n","\n","d = './drive/MyDrive/Colab-Notebooks/Search-Engine/Categories'\n","catdirs = [join(d,o) for o in listdir(d) if isdir(join(d,o))]\n","vocab = read_vocab()\n","vector_file = open('./drive/MyDrive/Colab-Notebooks/Search-Engine/category_vectors.txt','w')\n","\n","for cat in catdirs:\n","    print(cat)\n","    vec = np.zeros(len(vocab))\n","    f = open(join(cat,'tf_idfs.txt'),'r')\n","    for l in f:\n","        l = l.rstrip('\\n')\n","        ngram = ' '.join([i for i in l.split()[:-1]])\n","        tf_idf = float(l.split()[-1])\n","        pos = vocab.index(ngram)\n","        vec[pos] = tf_idf\n","    f.close()\n","\n","    vector_file.write(cat+' '+' '.join([str(v) for v in vec])+'\\n')\n","vector_file.close()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d0f4b4Z3bzqu","executionInfo":{"status":"ok","timestamp":1669686725288,"user_tz":300,"elapsed":365,"user":{"displayName":"zulkaida akbar","userId":"03264843683657800588"}},"outputId":"1a697488-39d0-4f5d-973f-191545a03042"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Brazilian_telenovelas\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Cold_War_films\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Fauna_of_Zimbabwe\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Freeware_games\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/International_law\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Landscape_painters\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Members_of_the_Chinese_Academy_of_Sciences\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Musicals_based_on_novels\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Particle_physics\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/21st-century_women_mathematicians\n"]}]},{"cell_type":"code","source":["#USAGE: python3 classify.py <query_file>\n","import sys\n","import numpy as np\n","from math import sqrt\n","\n","\n","def cosine_similarity(v1, v2):\n","    num = np.dot(v1, v2)\n","    den_a = np.dot(v1, v1)\n","    den_b = np.dot(v2, v2)\n","    return num / (sqrt(den_a) * sqrt(den_b))\n","\n","def read_vocab():\n","    with open('./drive/MyDrive/Colab-Notebooks/Search-Engine/vocab_file.txt','r') as f:\n","        vocab = f.read().splitlines()\n","    return vocab\n","\n","def read_queries(query_file):\n","    with open(query_file) as f:\n","        queries = f.read().splitlines()\n","    return queries\n","\n","def read_category_vectors():\n","    vectors = {}\n","    f = open('./drive/MyDrive/Colab-Notebooks/Search-Engine/category_vectors.txt','r')\n","    for l in f:\n","        l = l.rstrip('\\n')\n","        fields = l.split()\n","        cat = fields[0]\n","        vec = np.array([float(v) for v in fields[1:]])\n","        vectors[cat] = vec\n","    return vectors\n","\n","def get_ngrams(l,n):\n","    l = l.lower()\n","    ngrams = {}\n","    for i in range(0,len(l)-n+1):\n","        ngram = l[i:i+n]\n","        if ngram in ngrams:\n","            ngrams[ngram]+=1\n","        else:\n","            ngrams[ngram]=1\n","    return ngrams\n","\n","def normalise_tfs(tfs,total):\n","    for k,v in tfs.items():\n","        tfs[k] = v / total\n","    return tfs\n","\n","def mk_vector(vocab,tfs):\n","    vec = np.zeros(len(vocab))\n","    for t,f in tfs.items():\n","        if t in vocab:\n","            pos = vocab.index(t)\n","            vec[pos] = f\n","    return vec\n","\n","vocab = read_vocab()\n","print(len(vocab))\n","vectors = read_category_vectors()\n","#queries = read_queries(sys.argv[1])\n","queries = open('./drive/MyDrive/Colab-Notebooks/Search-Engine/query_file.txt','r')\n","\n","for q in queries:\n","    print(\"\\nQUERY:\",q)\n","    ngrams = {}\n","    cosines = {}\n","    for i in range(4,7):\n","        n = get_ngrams(q,i)\n","        ngrams = {**ngrams, **n}\n","    qvec = mk_vector(vocab,ngrams)\n","    for cat,vec in vectors.items():\n","        cosines[cat] = cosine_similarity(vec,qvec)\n","    for cat in sorted(cosines, key=cosines.get, reverse=True):\n","        print(cat,cosines[cat])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ckvDtAcsdEiU","executionInfo":{"status":"ok","timestamp":1669686794755,"user_tz":300,"elapsed":172,"user":{"displayName":"zulkaida akbar","userId":"03264843683657800588"}},"outputId":"94c1cfa2-01b7-4643-b66e-83e8ff7bb845"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["1000\n","\n","QUERY: animals of zimbabwe\n","\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Fauna_of_Zimbabwe 0.0962172268166385\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Cold_War_films 0.027295703295014084\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Brazilian_telenovelas 0.0\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Freeware_games 0.0\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/International_law 0.0\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Landscape_painters 0.0\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Members_of_the_Chinese_Academy_of_Sciences 0.0\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Musicals_based_on_novels 0.0\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Particle_physics 0.0\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/21st-century_women_mathematicians 0.0\n","\n","QUERY: famous mathematicians\n","\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/21st-century_women_mathematicians 0.8770452810526391\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Members_of_the_Chinese_Academy_of_Sciences 0.22191339717307546\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Particle_physics 0.022071807684747754\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Freeware_games 0.0006301704687737412\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/International_law 0.0005240612162921583\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Brazilian_telenovelas 0.0\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Cold_War_films 0.0\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Fauna_of_Zimbabwe 0.0\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Landscape_painters 0.0\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Musicals_based_on_novels 0.0\n","\n","QUERY: broadway musicals\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Musicals_based_on_novels 0.7191498522182114\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Cold_War_films 0.01108330439669752\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Freeware_games 0.0029106327182549774\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/21st-century_women_mathematicians 0.001655107347195565\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Brazilian_telenovelas 0.0014134100782492956\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Fauna_of_Zimbabwe 0.0\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/International_law 0.0\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Landscape_painters 0.0\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Members_of_the_Chinese_Academy_of_Sciences 0.0\n","./drive/MyDrive/Colab-Notebooks/Search-Engine/Categories/Particle_physics 0.0\n"]}]}]}